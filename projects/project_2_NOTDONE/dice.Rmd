---
title: "Rolling Dice to Exemplify the Central Limit Theorem"
output:
  html_notebook:
    code_folding: hide
---

### Motivation: roll 100 dice with a single spin

Can we make a spinner that is equivalent to rolling (and adding) 100 dice? Yes, and we will use the Central Limit Theorem to do so.

### Population Parameters

A 6-sided dice is equally likely to roll 1, 2, 3, 4, 5, or 6. This is a [discrete-uniform distribution](https://en.wikipedia.org/wiki/Discrete_uniform_distribution){target="blank"} with lower bound $a=1$ and upper bound $b=6$. We can use the formulas that are specific to discrete-uniform distributions:
```{r}
mu = (1+6)/2
sigma = sqrt(35/12)
```
$$\mu = \frac{a+b}{2} = \frac{1+6}{2} = 3.5 $$
$$\sigma = \sqrt{\frac{(b-a+1)^2-1}{12}} = \sqrt{\frac{(6-1+1)^2-1}{12}} = \sqrt{\frac{36-1}{12}} = `r sqrt(35/12)`$$

### Sampling distribution

The **total** of $n=100$ rolls **is itself a random variable**. We think the total will be near 350, but we won't be surprised if the total is 340; however if the total is 200, something weird has happened. We want a precise description of the possible totals and their probabilities. 

We say that the unknown total follows a [sampling distribution](https://en.wikipedia.org/wiki/Sampling_distribution){target="blank"}. 

### Central limit theorem

The central limit theorem tells us the unknown total of 100 rolls should be approximately normal (in its probability distribution). We can use formulas to determine the average and standard deviation of the total's sampling distribution.
$$\mu_\text{total} = n\mu = (100)(3.5) = 350$$
$$\sigma_\text{total} = \sqrt{n} \sigma = \sqrt{100}(`r sqrt(35/12)`) = `r 10*sqrt(35/12)` $$
Some people call $\sigma_\text{total}$ the [standard error](https://en.wikipedia.org/wiki/Standard_error){target="blank"} of the total, but it is also the standard deviation of the total's sampling distribution.

### Continuity correction

The total of 100 rolls will be an integer, but a normal distribution produces continuous values. To deal with this, we make a [continuity correction](https://en.wikipedia.org/wiki/Continuity_correction){target="blank"}. In other words, we round the normal value to get the appropriate total.

```{r}
x = rnorm(1, 350, 17.078)
x2 = round(x)
```

For example, if the normal distribution gave `r x`, we would count that as simulating a total of `r x2`.

### Making the spinner

Each half integer $x$ between 310 and 390 can be turned into a $z$-score.
$$z = \frac{x-350}{17.078} $$
For example, the boundary between 341 and 342 is 341.5, which is converted to a $z$-score.
$$z = \frac{341.5-350}{17.078} = `r (341.5-350)/17.078`$$
This $z$-score can be converted to a cumulative probability by using `pnorm` in R or `norm.dist` in a spreadsheet. 
```{r}
cat(pnorm(-0.4977164))
```
This cumulative probability dictates how far around the circle the boundary should be placed.

```{r fig.width=10,fig.height=10,echo=F}
xs = seq(299.5,400.5,1)
ps = pnorm(xs,350,17.078)
theta = seq(0,2*pi,length.out=1000)
par(mar=c(0,0,0,0))
plot(sin(theta),cos(theta),type="l",xlim=c(-1.1,1.1),ylim=c(-1.1,1.1))
for(i in 1:length(ps)){
  lines(c(0.7,1)*sin(ps[i]*2*pi),c(0.7,1)*cos(ps[i]*2*pi),lwd=0.5)
}
for(i in 1:(length(ps)-1)){
  if(i%%5==1){
    thetas = seq(2*pi*ps[i],2*pi*ps[i+1],0.001)
    polygon(c(sin(thetas),0.7*sin(rev(thetas))),
            c(cos(thetas),0.7*cos(rev(thetas))),
            col=rgb(0,0,0,0.5),border=NA)
  }
  ppp = (ps[i]+ps[i+1])/2
  dp = ps[i+1]-ps[i]
  xt = 0.85*sin(ppp*2*pi)
  yt = 0.85*cos(ppp*2*pi)
  text(xt,yt,299+i,cex=dp*80,srt=(180-360*ppp)%%180-90)
}
lines(0.7*sin(theta),0.7*cos(theta))
for(i in -3:3){
  lines(c(0.5,0.7)*sin(pnorm(i)*2*pi),c(0.5,0.7)*cos(pnorm(i)*2*pi))
  if(abs(i)<3){
    text(0.45*sin(pnorm(i)*2*pi),0.45*cos(pnorm(i)*2*pi),i,srt=(180-360*pnorm(i))%%180-90,cex=1)
  }
}
for(i in seq(-2.5,2.5,1)){
  lines(c(0.55,0.7)*sin(pnorm(i)*2*pi),c(0.55,0.7)*cos(pnorm(i)*2*pi))
  if(abs(i)<2){
    text(0.5*sin(pnorm(i)*2*pi),0.5*cos(pnorm(i)*2*pi),i,srt=(180-360*pnorm(i))%%180-90,cex=0.6)
  }
}
for(i in seq(-3,3,0.1)){
  lines(c(0.6,0.7)*sin(pnorm(i)*2*pi),c(0.6,0.7)*cos(pnorm(i)*2*pi))
  if(abs(i)<1.7 & i%%0.5 != 0){
    text(0.55*sin(pnorm(i)*2*pi),0.55*cos(pnorm(i)*2*pi),round(i,2),srt=(180-360*pnorm(i))%%180-90,cex=0.5)
  }
}
for(i in 1:10){
  p = i/10
  lines(c(1,1.1)*sin(p*2*pi),c(1,1.1)*cos(p*2*pi),lwd=2)
}
for(i in 1:20){
  p = i/20
  lines(c(1,1.05)*sin(p*2*pi),c(1,1.05)*cos(p*2*pi))
}
for(i in 1:100){
  p = i/100
  lines(c(1,1.02)*sin(p*2*pi),c(1,1.02)*cos(p*2*pi))
}
polygon(0.05*sin(theta),0.05*cos(theta),col="red")
arrows(-0.7,-0.1,0.7,0.1,col="black",lwd=10)
arrows(-0.7,-0.1,0.7,0.1,col="red",lwd=8)
polygon(0.01*sin(theta),0.01*cos(theta),col="red")
```
This spinner shows three different distributions. The center distribution is the [standard normal distribution](https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution){target="blank"}. The intermediate distribution is a rounded normal distribution with mean 350 and standard deviation of 17.078; this intermediate distribution simulates rolling 100 6-sided dice and adding the values. The outer distribution is the [standard uniform distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distribution#Standard_uniform){target="blank"}, which also indicates the [cumulative probability](https://en.wikipedia.org/wiki/Cumulative_distribution_function){target="blank"}.

You may wonder about the blank section on the top of the spinner. The regions were getting too small to read the numbers. This issue could be dealt with by using a series a spinners, where the blank section would indicate that the next spinner should be spun. However, we don't plan to actually use this physical spinner; it is meant to help visualize the distribution.

Of course, many computers can generate standard normal random values. Instead of using a physical spinner, a computer works well. For example, in R, you can use the `rnorm` function to generate a random $z$ score.
```{r}
z = rnorm(1)
cat(z)
```
You can convert this into a dice total.
$$x = 350+(z)(17.078) = 350+(`r z`)(17.078) = `r 350+z*17.078` \approx `r round(350+z*17.078)`$$

### Checking the Central Limit Theorem

We can also use a computer to simulate rolling 100 dice more directly (with 100 discrete-uniform variables). We can use this to check the accuracy of the Central Limit Theorem in this case.

```{r}
xx = sample(1:6,100*1000000,T)
xx = matrix(xx,ncol=100)
totals = apply(xx,1,sum)
```
```{r fig.width=12,fig.height=6}
hist(totals,breaks=seq(min(totals)-0.5,max(totals)+0.5,1),freq=F,main="A million repetitions of adding 100 dice")
xx2 = seq(min(totals),max(totals),0.1)
lines(xx2,dnorm(xx2,350,17.078),col=rgb(1,0.5,0.5,1))
xx3 = seq(min(totals),max(totals),1)
pxx3 = pnorm(xx3+0.5,350,17.078)-pnorm(xx3-0.5,350,17.078)
# for(i in 1:length(xx3)){
#   lines(c(xx3[i],xx3[i]),c(0,pxx3[i]),col="red")
# }
```

The red curve indicates the prediction from the central limit theorem (normal with mean 350 and standard deviation 17.078). The bars indicate the results of the simulation (rolling a million repetitions of 100 dice).

### Average of 100 rolls

To simulate the **average** of 100 rolls, we could use the above spinner and divide the result by 100. Thus, the sampling distribution of the **average** of 100 rolls is approximately normal with an average of 3.5 and a standard deviation of 0.17078.

```{r fig.width=10,fig.height=10,echo=F}
xs = seq(299.5,400.5,1)
ps = pnorm(xs,350,17.078)
theta = seq(0,2*pi,length.out=1000)
par(mar=c(0,0,0,0))
plot(sin(theta),cos(theta),type="l",xlim=c(-1.1,1.1),ylim=c(-1.1,1.1))
for(i in 1:length(ps)){
  lines(c(0.7,1)*sin(ps[i]*2*pi),c(0.7,1)*cos(ps[i]*2*pi),lwd=0.5)
}
for(i in 1:(length(ps)-1)){
  if(i%%5==1){
    thetas = seq(2*pi*ps[i],2*pi*ps[i+1],0.001)
    polygon(c(sin(thetas),0.7*sin(rev(thetas))),
            c(cos(thetas),0.7*cos(rev(thetas))),
            col=rgb(0,0,0,0.5),border=NA)
  }
  ppp = (ps[i]+ps[i+1])/2
  dp = ps[i+1]-ps[i]
  xt = 0.85*sin(ppp*2*pi)
  yt = 0.85*cos(ppp*2*pi)
  text(xt,yt,(299+i)/100,cex=dp*80,srt=(180-360*ppp)%%180-90)
}
lines(0.7*sin(theta),0.7*cos(theta))
for(i in -3:3){
  lines(c(0.5,0.7)*sin(pnorm(i)*2*pi),c(0.5,0.7)*cos(pnorm(i)*2*pi))
  if(abs(i)<3){
    text(0.45*sin(pnorm(i)*2*pi),0.45*cos(pnorm(i)*2*pi),i,srt=(180-360*pnorm(i))%%180-90,cex=1)
  }
}
for(i in seq(-2.5,2.5,1)){
  lines(c(0.55,0.7)*sin(pnorm(i)*2*pi),c(0.55,0.7)*cos(pnorm(i)*2*pi))
  if(abs(i)<2){
    text(0.5*sin(pnorm(i)*2*pi),0.5*cos(pnorm(i)*2*pi),i,srt=(180-360*pnorm(i))%%180-90,cex=0.6)
  }
}
for(i in seq(-3,3,0.1)){
  lines(c(0.6,0.7)*sin(pnorm(i)*2*pi),c(0.6,0.7)*cos(pnorm(i)*2*pi))
  if(abs(i)<1.7 & i%%0.5 != 0){
    text(0.55*sin(pnorm(i)*2*pi),0.55*cos(pnorm(i)*2*pi),round(i,2),srt=(180-360*pnorm(i))%%180-90,cex=0.5)
  }
}
for(i in 1:10){
  p = i/10
  lines(c(1,1.1)*sin(p*2*pi),c(1,1.1)*cos(p*2*pi),lwd=2)
}
for(i in 1:20){
  p = i/20
  lines(c(1,1.05)*sin(p*2*pi),c(1,1.05)*cos(p*2*pi))
}
for(i in 1:100){
  p = i/100
  lines(c(1,1.02)*sin(p*2*pi),c(1,1.02)*cos(p*2*pi))
}
polygon(0.05*sin(theta),0.05*cos(theta),col="red")
arrows(-0.7,-0.1,0.7,0.1,col="black",lwd=10)
arrows(-0.7,-0.1,0.7,0.1,col="red",lwd=8)
polygon(0.01*sin(theta),0.01*cos(theta),col="red")
```

### Running average

To understand the central limit theorem, it can be helpful to visualize [running averages](https://en.wikipedia.org/wiki/Moving_average#Cumulative_moving_average){target="blank"}.

A running average depicts the progression of the sample mean as the sample size increases (up to 100 in our case). Let's roll 100 dice and show the running average.

```{r fig.width=6,fig.height=4}
x4 = sample(1:6,100,T)
cat(x4)
n = 1:100
running_average = cumsum(x4)/n
plot(n,running_average,type="l",ylim=c(1,6))
abline(h=3.5,lty=2,col="red")
```

Because the first roll was `r x4[1]`, the running average starts at (1, `r x4[1]`). The second roll was `r x4[2]`, so the running average then moves to (2, `r (x4[1]+x4[2])/2`). The third roll was `r x4[3]`, so then the running average moved to (3, `r mean(x4[1:3])`). This is because $\frac{`r x4[1]`+`r x4[2]`+`r x4[3]`}{3}=`r mean(x4[1:3])`$. And so on. 

We see the running average is volatile while $n$ is small, but smooths out and approaches the population average (shown as a dotted red line) as $n$ increases. This exemplifies the [law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers){target="blank"}.

If we overlay many running averages, we will see a (sideways) funnel shape, funneling toward the population mean. At each $n$, we can calculate an interval of typical means.
$$\text{interval of typical means} = \left(\mu-\frac{2\sigma}{\sqrt{n}}\,,\,\,\mu+\frac{2\sigma}{\sqrt{n}} \right)$$
These intervals predict the funnel shape.

```{r fig.width=6,fig.height=4}
cumave = function(x){
  return(cumsum(x)/1:length(x))
}
x5 = sample(1:6,100*200,T)
x5 = matrix(x5,ncol=100)
n = 1:100
running_average5 = apply(x5,1,cumave)
plot(0,0,type="n",xlim=c(0,100),ylim=c(1,6),xlab="n",
     ylab="running average",main="200 repetitions of rolling 100 dice with running average")
for(i in 1:length(x5[1,])){
  lines(n,running_average5[,i],col=rgb(0,0,0,0.2))
}
abline(h=3.5,lty=2,col="red")
lines(n,mu-2*sigma/sqrt(n),col="green")
text(90,2.5,expression(mu-2*sigma/sqrt(n)),col="green")
lines(n,mu+2*sigma/sqrt(n),col="blue")
text(90,4.5,expression(mu+2*sigma/sqrt(n)),col="blue")
```

More precisely, with enough repetitions, at any $n$ larger than 30 (or so), we expect 95% of the sample means to be within 2 standard error of the population mean. We also expect 68% of the sample means within 1 standard error. 

All of this is because of the central limit theorem: when $\bar{x}$ will be measured, we expect $\frac{\bar{x}-\mu}{\sigma/\sqrt{n}}$ to follow the standard normal distribution (as long as $n$ is large enough). 




